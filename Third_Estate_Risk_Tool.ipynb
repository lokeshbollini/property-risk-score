{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPNF9j+q1JHdJg205TcPmW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiragkhachane/real-estate-anaytics/blob/main/Third_Estate_Risk_Tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O61ArLAKAto"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load the dataset with latin-1 encoding\n",
        "data = pd.read_csv(\"MasterData.csv\")\n",
        "\n",
        "# Select relevant columns\n",
        "selected_columns = ['Print Key', 'Code_Description', 'Comments', 'Prop Class Description', 'Code', 'Owner1', 'Owner2',\n",
        "                   'Neighborhood', 'Dist Name', 'Year Built', 'Distinct count of Case Reference', '#ofBaths', '#ofBeds',\n",
        "                   'GEOID20 blockgroup', 'Numberof Units', 'Total Value', 'Mortgage', 'CostPerSQFT']\n",
        "data = data[selected_columns]\n",
        "\n",
        "# Drop rows with missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Treat 'Print Key' as string and target variable\n",
        "data['Print Key'] = data['Print Key'].astype(str)\n",
        "\n",
        "# Preprocess numerical columns\n",
        "numerical_columns = ['Year Built', 'Distinct count of Case Reference', '#ofBaths', '#ofBeds', 'Numberof Units', 'Total Value', 'Mortgage', 'CostPerSQFT']\n",
        "for column in numerical_columns:\n",
        "    data[column] = data[column].replace(',', '', regex=True).astype(float)  # Remove commas and convert to float\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize and encode textual data\n",
        "encoded_texts = tokenizer(data['Code_Description'].tolist(), padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Extract contextualized embeddings from BERT model\n",
        "bert_outputs = bert_model(encoded_texts)\n",
        "bert_embeddings = bert_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Define custom weights for each attribute\n",
        "custom_weights = {\n",
        "    'Code_Description': 0.7,\n",
        "    'Comments': 0.4,\n",
        "    'Prop Class Description': 0.1,\n",
        "    'Code': 0.05,\n",
        "    'Owner1': 0.05,\n",
        "    'Owner2': 0.05,\n",
        "    'Neighborhood': 0.1,\n",
        "    'Dist Name': 0.05,\n",
        "    'Year Built': 0.05,\n",
        "    'Distinct count of Case Reference': 0.4,\n",
        "    '#ofBaths': 0.05,\n",
        "    '#ofBeds': 0.05,\n",
        "    'GEOID20 blockgroup': 0.1,\n",
        "    'Numberof Units': 0.1,\n",
        "    'Total Value': 0.2,\n",
        "    'Mortgage': 0.8,\n",
        "    'CostPerSQFT': 0.2\n",
        "}\n",
        "\n",
        "# Combine BERT embeddings with other features\n",
        "features = tf.concat([bert_embeddings,\n",
        "                      data[['Year Built', 'Distinct count of Case Reference', '#ofBaths', '#ofBeds', 'GEOID20 blockgroup',\n",
        "                            'Numberof Units', 'Total Value', 'Mortgage', 'CostPerSQFT']].values], axis=1)\n",
        "\n",
        "# Calculate custom weights for features\n",
        "custom_weighted_features = features.numpy().copy()\n",
        "for feature, weight in custom_weights.items():\n",
        "    if feature == 'Code_Description':\n",
        "        feature_index = 0\n",
        "    else:\n",
        "        feature_index = data.columns.get_loc(feature) - len(encoded_texts.keys())  # Adjust index for BERT embeddings\n",
        "    custom_weighted_features[:, feature_index] *= weight\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "custom_weighted_features = scaler.fit_transform(custom_weighted_features)\n",
        "\n",
        "# Define target variable\n",
        "y = data['Print Key']\n",
        "\n",
        "# Initialize label encoder for target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Initialize Random Forest Regression model\n",
        "rf_model = RandomForestRegressor(n_estimators=100)\n",
        "\n",
        "# Train the Random Forest Regression model on the entire dataset\n",
        "rf_model.fit(custom_weighted_features, y_encoded)\n",
        "\n",
        "# Predict risk scores for all print keys\n",
        "all_predictions = rf_model.predict(custom_weighted_features)\n",
        "\n",
        "# Create a DataFrame with print keys and predicted risk scores\n",
        "predictions_df = pd.DataFrame({'Print Key': data['Print Key'], 'Predicted_Risk_Score': all_predictions})\n",
        "\n",
        "# Export predictions to a CSV file\n",
        "predictions_df.to_csv('predicted_risk_scores2.csv', index=False)"
      ]
    }
  ]
}